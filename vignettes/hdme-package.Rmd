---
title: "The hdme package: regression methods for high-dimensional data with measurement error"
author: "Øystein Sørensen"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: ../inst/REFERENCES.bib
vignette: >
  %\VignetteIndexEntry{The hdme package: regression methods for high-dimensional data with measurement error}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width=6
)
```

The `hdme` package contains algorithms for regression problems with measurement error when the number of covariates $p$ is on the same order as the number of samples $n$, or even larger. 

# Measurement Error in Regression

## Classical Measurement Error Problems
Consider a linear regression model, $y = X\beta + \epsilon$, where $y$ is an $n$-dimensional response vector, $X$ is an $n \times p$ design matrix, and $\epsilon$ is normally distributed error. With $n>p$ (and $X$ positive definite), unbiased regression coefficients are obtained as 

$$
\hat{\beta} = (X^{T}X )^{-1} X^{T} y.
$$ 

In many cases, however, the true covariates $X$ are not directly observed. Instead, we have noise measurements

$$ W = X + U, $$

where $W$ is the $n\times p$ measurement matrix, and $U$ is the $n\times p$ matrix of measurement errors. Assume for the moment that the $n$ rows of $U$, $u_{i}$ are identically and independently distributed with covariance matrix $\Sigma_{uu}$, i.e.,

$$ u_{i} \sim N(0, \Sigma_{uu}), ~ i = 1,\dots,n. $$

Since we do not have measurements of $X$, using a classical linear regression model now yields coefficient estimate

$$
\hat{\beta}_{naive} = (W^{T}W )^{-1} W^{T} y,
$$
which is referred to as the *naive* estimate in the measurement error literature. Naive estimates are typically biased. For example, when all measurement errors are uncorrelated and each have variance $\sigma_{u}$, i.e., $\Sigma_{uu} = \sigma_{u} I_{p\times p}$, the expected value of $\hat{\beta}$ is

$$
E(\hat{\beta}_{naive}) = \frac{\sigma_{x}^{2}}{\sigma_{u}^{2} + \sigma_{x}^2} \beta = \gamma \beta,
$$
where $\gamma$ is the attenuation factor. Unbiased estimates for the case of linear regression can be obtained using the corrected estimator
$$
\hat{\beta}_{corr} = (W^{T}W - \Sigma_{uu} )^{-1} W^{T} y.
$$
An estimate $\hat{\Sigma}_{uu}$ of $\Sigma_{uu}$ can be typically obtained with replicate measurements. 

Similar results hold for generalized linear models, e.g., logistic and Poisson regression: Measurement error typically leads to bias in the naive estimates, and correction methods can be used to obtained unbiased estimates. We refer to the book @carroll2006 for a thorough introduction to measurement error modeling.


## Measurement Error in High-Dimensional Regression Problems
Now consider the same setup as in the previous section, but with high-dimensional data. That is, the number of covariates $p$ is either on the same order as $n$, or even larger than $n$. In this case, even in the absence of measurement error, regularization is typically needed (see, e.g., Chapter 3 of @hastie2009). 

### The lasso
The lasso (@tibshirani1996) performs $L1$-regularization, which induces sparsity. A popular `R` implementation can be found in the `glmnet` package (@simon2011). For linear regression models, the lasso finds $\hat{\beta}$ minimizing the loss
$$
L(\beta) = \|y - X \beta\|^{2} + \lambda \|\beta\|_{1},
$$
where $\lambda$ is a regularization parameter. The lasso can also be formulated as a constrained optimization problem,
$$
\text{minimize }  ~\|y - X\beta\|^{2}~  \text{ subject to }~ \|\beta\|_{1} \leq R,
$$
where $R$ is in a one-to-one relationship with $\lambda$.

A lot of effort has been put on understanding the statistical properties of the lasso, both in the classical $p<n$ case and in the high-dimensional $p>n$ case, summarized in @buhlmann2011. The results are typically either probabilistic upper bounds on or asymptotic limits for

* Prediction error $E(\|y_{new} - X_{new}\hat{\beta}\|)$ where $(X_{new}, y_{new})$ are a new sample of data.
* Sum of squared estimation error $\|\hat{\beta} - \beta\|_{2}^{2}$ or sum of absolute estimation error $\|\hat{\beta} - \beta\|_{1}$
* Covariate selection: If true coefficients in the index set $J \subseteq \{1, \dots, p\}$ are nonzero, while the rest are zero, i.e., $\beta_{J} \neq 0$ and $\beta_{J^{C}} = 0$, under which conditions will the lasso recover the true set of relevant covariates while correctly setting the irrelevant covariates to zero?

The impact of measurement error in the lasso for linear models has been studied by @sorensen2015. The authors show that estimation of $\hat{\beta}$ in the asymptotic limit suffers the same bias as for a multivariate linear model described above. Consistent covariate selection, on the other hand, requires stricter conditions than in the case without measurement error.

Using the `glmnet` package, we can illustrate the impact of measurement error in a small experiment.

First we set up the parameters and generate random data:

```{r}
set.seed(1000)
# Number of samples
n <- 100
# Number of covariates
p <- 500
# Number of nonzero coefficients
s <- 5
# True coefficient vector
beta <- c(rnorm(n = s, sd = 2), rep(0, p - s))
# Standard deviation of covariates
sdX <- 1
# Standard deviation of measurement error
sdU <- 0.5
# Standard deviation of residual
sdEpsilon <- 0.1
# Generate the covariates
X <- matrix(rnorm(n * p, sd = sdX), nrow = n, ncol = p)
# Generate the response
y <- X %*% beta + rnorm(n, sd = sdEpsilon)
# Generate the measurements by adding measurement error
W <- X + matrix(rnorm(n * p, sd = sdU), nrow = n, ncol = p)
```

Next we run the lasso with cross-validation on the true covariates and on the noisy measurements. We pick that coefficient estimate at $\lambda_{1se}$, i.e., on standard error on the sparse side of the cross-validation minimum. This is the default of the `coef` function for objects of class `cv.glmnet`.

```{r, message=FALSE}
library(glmnet)
library(tidyverse)
# Lasso with cross-validation on data without measurement error
fit1 <- cv.glmnet(X, y)
# Lasso with cross-validation on data with measurement error
fit2 <- cv.glmnet(W, y)
# Create a data frame with results ([-1] because we drop the intercept)
lassoEstimates <- tibble(
  index = rep(1:p, times = 3),
  beta = c(beta, as.numeric(coef(fit1)[-1]), coef(fit2)[-1]),
  label = c(rep("True values", p), rep("No measurement error", p), rep("Measurement error", p))
  )

```

By plotting all the estimated regression coefficients, we see that when the data are subject to measurement error, the number of false postives may increase. Note that this is not necessarily the case for all choices of parameters.

```{r}
ggplot(lassoEstimates, aes(x = index, y = beta, color = label)) +
  geom_point() +
  xlab("p") +
  theme(legend.title=element_blank()) + 
  ggtitle("Measurement error leading to false positives")
```

We can also focus on the `r s` parameters which are truly nonzero. In this case, we see that in the absence of measurement error, the lasso estimates values quite close to truth. With measurement error, on the other hand, the attenuation is quite clear: the estimates are biased toward zero.

```{r}
estimatesOfNonzero <- lassoEstimates %>% 
  spread(key = label, value = beta) %>% 
  filter(`True values` != 0) %>% 
  gather(key = label, value = beta, -index)

ggplot(estimatesOfNonzero, aes(x = index, y = beta, color = label)) +
  geom_point() +
  xlab("p") +
  theme(legend.title=element_blank()) + 
  ggtitle("Measurement error leading to attenuation")
```

### The Dantzig Selector


# Corrected Lasso
When an estimate of the measurement error covariance matrix $\Sigma_{uu}$ is available, a corrected lasso can be defined as minimizing the loss
$$
\text{minimize } ~ L(\beta) = \|y - W \beta \|^{2} - \beta^{T} \Sigma_{uu} \beta + \lambda \|\beta\|_{1} ~\text{ subject to } \|\beta\|_{1} \leq R.
$$
Because we subtract the positive semidefinite matrix $\Sigma_{uu}$ from the convex function $\|y - W\beta\|^{2}$, this corrected lasso may be non-convex. In fact, when $p>n$ is is always non-convex. Hence, in order to avoid non-trivial solution, we must constrain the solution to lie in some L1-ball with radius $R$. Since this problem involves two regularization parameters, $\lambda$ and $R$, it is more convenient to use the


## Corrected Lasso for Linear Regression

## Corrected Lasso for Generalized Linear Models

## Model Tuning

- Cross validation

# Matrix Uncertainty Selector

## Matrix Uncertainty Selector

## Generalized Matrix Uncertainty Selector

## Model Tuning

- Elbow rule

# Interpretation of Results

## Plotting Important Features

## Sensitivity Analysis

- Investigate the impact of measurement error
- Compare the output of standard methods to the outputs of corrected methods.


# References

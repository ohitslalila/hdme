---
title: "The hdme package: regression methods for high-dimensional data with measurement error"
author: "Øystein Sørensen"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: ../inst/REFERENCES.bib
vignette: >
  %\VignetteIndexEntry{The hdme package: regression methods for high-dimensional data with measurement error}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width=6
)
```

The `hdme` package contains algorithms for regression problems with measurement error when the number of covariates $p$ is on the same order as the number of samples $n$, or even larger. 

# Measurement Error in Regression

## Classical Measurement Error Problems
Consider a linear regression model, $y = X\beta + \epsilon$, where $y$ is an $n$-dimensional response vector, $X$ is an $n \times p$ design matrix, and $\epsilon$ is normally distributed error. With $n>p$ (and $X$ positive definite), unbiased regression coefficients are obtained as 

$$
\hat{\beta} = (X^{T}X )^{-1} X^{T} y.
$$ 

In many cases, however, the true covariates $X$ are not directly observed. Instead, we have noisy measurements

$$ W = X + U, $$

where $W$ is the $n\times p$ measurement matrix, and $U$ is the $n\times p$ matrix of measurement errors. Assume for the moment that the $n$ rows of $U$, $u_{i}$ are identically and independently distributed with covariance matrix $\Sigma_{uu}$, i.e.,

$$ u_{i} \sim N(0, \Sigma_{uu}), ~ i = 1,\dots,n. $$

Since we do not have measurements of $X$, using a classical linear regression model now yields coefficient estimates

$$
\hat{\beta}_{naive} = (W^{T}W )^{-1} W^{T} y,
$$
which is referred to as the *naive* estimate in the measurement error literature. Naive estimates are typically biased. For example, when all measurement errors are uncorrelated and each have variance $\sigma_{u}$, i.e., $\Sigma_{uu} = \sigma_{u} I_{p\times p}$, the expected value of $\hat{\beta}$ is

$$
E(\hat{\beta}_{naive}) = \frac{\sigma_{x}^{2}}{\sigma_{u}^{2} + \sigma_{x}^2} \beta = \gamma \beta,
$$
where $\gamma$ is the attenuation factor. Unbiased estimates for the case of linear regression can be obtained by minimizing the corrected loss function
$$
L_{corr}(\beta) = \|y - W\beta\|^{2} - \beta^{T}\Sigma_{uu} \beta.
$$
This $L_{corr}(\beta)$ is not always convex. If the Hessian $W^{T}W - \Sigma_{uu}$ is positive definite, $L_{corr}(\beta)$ is convex, and the estimates can be found using

$$
\hat{\beta}_{corr} = (W^{T}W - \Sigma_{uu} )^{-1} W^{T} y.
$$
Otherwise, iteration methods must be used to find a minimum of $L(\beta)$.

An estimate $\hat{\Sigma}_{uu}$ of $\Sigma_{uu}$ can be typically obtained with replicate measurements. 

Similar results hold for generalized linear models, e.g., logistic and Poisson regression: Measurement error typically leads to bias in the naive estimates, and correction methods can be used to obtained unbiased estimates. We refer to the book @carroll2006 for a thorough introduction to measurement error modeling.


## Measurement Error in High-Dimensional Regression Problems
Now consider the same setup as in the previous section, but with high-dimensional data. That is, the number of covariates $p$ is either on the same order as $n$, or even larger than $n$. In this case, even in the absence of measurement error, regularization is typically needed (see, e.g., Chapter 3 of @hastie2009). 

### The lasso
The lasso (@tibshirani1996) performs $L1$-regularization, which induces sparsity. A popular `R` implementation can be found in the `glmnet` package (@simon2011). For linear regression models, the lasso finds $\hat{\beta}$ minimizing the loss
$$
L(\beta) = \|y - X \beta\|^{2} + \lambda \|\beta\|_{1},
$$
where $\lambda$ is a regularization parameter. The lasso can also be formulated as a constrained optimization problem,
$$
\text{minimize }  ~\|y - X\beta\|^{2}~  \text{ subject to }~ \|\beta\|_{1} \leq R,
$$
where $R$ is in a one-to-one relationship with $\lambda$.

A lot of effort has been put into understanding the statistical properties of the lasso, both in the classical $p<n$ case and in the high-dimensional $p>n$ case, summarized in @buhlmann2011. The results are typically either probabilistic upper bounds on or asymptotic limits for

* Prediction error $E(\|y_{new} - X_{new}\hat{\beta}\|)$ where $(X_{new}, y_{new})$ are a new sample of data.
* Sum of squared estimation error $\|\hat{\beta} - \beta\|_{2}^{2}$ or sum of absolute estimation error $\|\hat{\beta} - \beta\|_{1}$
* Covariate selection: If true coefficients in the index set $J \subseteq \{1, \dots, p\}$ are nonzero, while the rest are zero, i.e., $\beta_{J} \neq 0$ and $\beta_{J^{C}} = 0$, under which conditions will the lasso recover the true set of relevant covariates while correctly setting the irrelevant covariates to zero?

The impact of measurement error in the lasso for linear models has been studied by @sorensen2015. The authors show that estimation of $\hat{\beta}$ in the asymptotic limit suffers the same bias as for a multivariate linear model described above. Consistent covariate selection, on the other hand, requires stricter conditions than in the case without measurement error.

Using the `glmnet` package, we can illustrate the impact of measurement error in a small experiment.

First we set up the parameters and generate random data. In order to repeat the procedure, we create the function `create_example_data` for doing this.

```{r}
create_example_data <- function(n, p, s = 5, sdX = 1, sdU = 0.5, sdEpsilon = 0.1) {
  # True coefficient vector has s non-zero elements and p-s zero elements
  beta <- c(rnorm(n = s, sd = 2), rep(0, p - s))
  # Independent true covariates with mean zero and standard deviation sdX
  X <- matrix(rnorm(n * p, sd = sdX), nrow = n, ncol = p)
  # Response has standard deviation sdEpsilon, and the intercept is zero
  y <- X %*% beta + rnorm(n, sd = sdEpsilon)
  # The measurements W have mean X and standard deviation sdU. We assume uncorrelated measurment errors
  W <- X + matrix(rnorm(n * p, sd = sdU), nrow = n, ncol = p)
  
  return(list(X = X, W = W, y = y, beta = beta, sigmaUU = diag(p) * sdU))  
}
```

We then call the function to get the data. I use the `%<-%` from the `zeallot` package (@zeallot_package) to get all elements of the list directly from the function call. 

```{r}
library(zeallot)
n <- 100
p <- 500
set.seed(1000)
c(X, W, y, beta, sigmaUU) %<-% create_example_data(n, p)
```


Next we run the lasso with cross-validation on the true covariates and on the noisy measurements. We pick the coefficient estimate at $\lambda_{1se}$, i.e., on standard error on the sparse side of the cross-validation minimum. This is the default of the `coef` function for objects of class `cv.glmnet`.

```{r, message=FALSE}
library(glmnet)
library(tidyverse)
# Lasso with cross-validation on data without measurement error
fit1 <- cv.glmnet(X, y)
# Lasso with cross-validation on data with measurement error
fit2 <- cv.glmnet(W, y)
# Create a data frame with results ([-1] because we drop the intercept)
lassoEstimates <- tibble(
  index = rep(1:p, times = 3),
  beta = c(beta, as.numeric(coef(fit1)[-1]), coef(fit2)[-1]),
  label = c(rep("True values", p), rep("No measurement error", p), rep("Measurement error", p))
  )

```

By plotting all the estimated regression coefficients, we see that when the data are subject to measurement error, the number of false postives may increase. Note that this is not necessarily the case for all choices of parameters.

```{r}
ggplot(lassoEstimates, aes(x = index, y = beta, color = label)) +
  geom_point() +
  xlab("p") +
  theme(legend.title=element_blank()) + 
  ggtitle("Measurement error leading to false positives")
```

We can also focus on the `r sum(beta != 0)` parameters which are truly nonzero. In this case, we see that in the absence of measurement error, the lasso estimates values quite close to truth. With measurement error, on the other hand, the attenuation is quite clear: the estimates are biased toward zero.

```{r}
estimatesOfNonzero <- lassoEstimates %>% 
  spread(key = label, value = beta) %>% 
  filter(`True values` != 0) %>% 
  gather(key = label, value = beta, -index)

ggplot(estimatesOfNonzero, aes(x = index, y = beta, color = label)) +
  geom_point() +
  xlab("p") +
  theme(legend.title=element_blank()) + 
  ggtitle("Measurement error leading to attenuation")
```

### The Dantzig Selector


# Corrected Lasso

## Corrected Lasso for Linear Regression

When an estimate of the measurement error covariance matrix $\Sigma_{uu}$ is available, a corrected lasso can be defined as minimizing the loss
$$
\text{minimize } ~ L(\beta) = \|y - W \beta \|^{2} - \beta^{T} \Sigma_{uu} \beta + \lambda \|\beta\|_{1} ~\text{ subject to } \|\beta\|_{1} \leq R.
$$
Because we subtract the positive semidefinite matrix $\Sigma_{uu}$ from the convex function $\|y - W\beta\|^{2}$, this corrected lasso may be non-convex. In fact, when $p>n$ is is always non-convex. Hence, in order to avoid non-trivial solution, we must constrain the solution to lie in some L1-ball with radius $R$. Since this problem involves two regularization parameters, $\lambda$ and $R$, it is more convenient to use the constrained version of the lasso
$$
\text{minimize } ~ L(\beta) = \|y - W \beta \|^{2} - \beta^{T} \Sigma_{uu} \beta  ~\text{ subject to } \|\beta\|_{1} \leq R.
$$
@loh2012 analyze the properties of these two closely related versions of the lasso. They show that the bounds for the estimates of $\|\hat{\beta} - \beta\|_{2}^{2}$ and $\|\hat{\beta}-\beta\|_{1}$ are of the same order as for the standard lasso without measurement error, where $\hat{\beta}$ is the global minimum of the optimization problem. More remarkably, they show that despite non-convexity, under mild assumptions a projected gradient descent algorithm will converge with high probability to a local optimum which is very close to the global optimum. 

@sorensen2015 analyzed the covariate selection properties of the same model, and similarly showed that results very similar to those for covariate selection with the standard lasso in the absence of measurement, also hold for this corrected lasso.

### R Implementation
This package implements the projected gradient descent algorithm proposed by @loh2012. It can be found in the `fit_corrected_lasso` function. Using the `create_example_data` function defined above, we illustrate its use.

```{r}
library(hdme)
set.seed(1000)
# Generate example data
c(X, W, y, beta, sigmaUU) %<-% create_example_data(n, p)
# Fit the corrected lasso
corrected_fit <- fit_corrected_lasso(W = W, y = y, sigmaUU = sigmaUU)
```

The object returned is a `list` with class `corrected_lasso` and subclass `corrected_lasso_gaussian`.

```{r}
# Class of the object
class(corrected_fit)
# Object is a list
typeof(corrected_fit)
```

The arguments to the function are shown below.
```{r}
args(fit_corrected_lasso)
```

If the `radii` argument is not set, a naive lasso is run on the measurement error data, using cross-validation to find $\hat{\beta}_{naive}$. The maximum of $R$ is set to $R_{max}=2\|\hat{\beta}_{naive}\|_{1}$, i.e., the maximum possible solution to the corrected lasso is twice as large as the naive solution, as measured by the L1-norm. The minimum of $R$ is by default set to $R_{min} = 10^{-3}R_{max}$. The corrected lasso solution is then computed on an equally spaced grid between $R_{min}$ and $R_{max}$. The length of the grid is set by the `no_radii` argument, which by default equals $20$. 

The resulting estimates can be visualized using the `plot` function for objects of class `corrected_lasso`. Calling the function with no additional arguments returns a plot of the number of nonzero coefficients for each value of the constraint radius. This is equivalent to calling `plot(corrected_fit, type = "nonzero")`.

```{r}
plot(corrected_fit)
```

Instead using the additional argument `type = "path"` yields the full coefficient paths for the estimates for all values of the radius.

```{r}
plot(corrected_fit, type = "path")
```



## Corrected Lasso for Generalized Linear Models

## Model Tuning

- Cross validation

# Matrix Uncertainty Selector

## Matrix Uncertainty Selector

## Generalized Matrix Uncertainty Selector

## Model Tuning

- Elbow rule

# Interpretation of Results

## Plotting Important Features

## Sensitivity Analysis

- Investigate the impact of measurement error
- Compare the output of standard methods to the outputs of corrected methods.


# References
